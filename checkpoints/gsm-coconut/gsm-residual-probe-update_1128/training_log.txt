[2025-11-28 02:02:52] Config Loaded: {'project': 'test', 'save_path': './checkpoints/gsm-coconut', 'name': 'gsm-residual-probe-update_1128', 'only_eval': False, 'coconut': True, 'decoupling_mode': 'residual', 'cot': False, 'no_thoughts': False, 'no_cot': False, 'c_thought': 2, 'epochs_per_stage': 3, 'max_latent_stage': 3, 'pad_latent_to_max': True, 'save_only_improve': False, 'uniform_prob': 0.0, 'model_id': 'openai-community/gpt2', 'load_model_path': 'None', 'seed': 0, 'resume': 3, 'bf16': False, 'train_path': 'data/gsm_train.json', 'val_path': 'data/gsm_valid.json', 'reset_optimizer': True, 'batch_size_training': 32, 'debug': False, 'gradient_accumulation_steps': 1, 'num_epochs': 25, 'lr': 0.0001, 'weight_decay': 0.01}
[2025-11-28 02:02:52] Save Directory: ./checkpoints/gsm-coconut/gsm-residual-probe-update_1128
[2025-11-28 02:02:59] Initializing Coconut with mode: residual
[2025-11-28 02:02:59] Running FSDP on rank = 0
[2025-11-28 02:04:20] {"epoch": 4, "step": 1, "loss": 6.5008, "avg_alpha": 0.0, "avg_norm": 263.035, "avg_cosine": 0.9821, "reg_loss": 0.0}
[2025-11-28 02:05:41] {"epoch": 4, "step": 101, "loss": 1.1562, "avg_alpha": 0.0212, "avg_norm": 365.1255, "avg_cosine": 0.98, "reg_loss": 0.1217}
[2025-11-28 02:07:04] {"epoch": 4, "step": 201, "loss": 1.0574, "avg_alpha": 0.0123, "avg_norm": 362.1982, "avg_cosine": 0.9823, "reg_loss": 0.0594}
[2025-11-28 02:08:26] {"epoch": 4, "step": 301, "loss": 1.0451, "avg_alpha": 0.003, "avg_norm": 337.4937, "avg_cosine": 0.985, "reg_loss": 0.0117}
[2025-11-28 02:09:49] {"epoch": 4, "step": 401, "loss": 0.9221, "avg_alpha": 0.0028, "avg_norm": 299.7065, "avg_cosine": 0.9728, "reg_loss": 0.0112}
[2025-11-28 02:11:12] {"epoch": 4, "step": 501, "loss": 1.0435, "avg_alpha": 0.003, "avg_norm": 326.3096, "avg_cosine": 0.9837, "reg_loss": 0.0085}
[2025-11-28 02:12:36] {"epoch": 4, "step": 601, "loss": 0.8168, "avg_alpha": 0.0088, "avg_norm": 327.5361, "avg_cosine": 0.9775, "reg_loss": 0.0348}
[2025-11-28 02:14:01] {"epoch": 4, "step": 701, "loss": 0.8638, "avg_alpha": 0.0149, "avg_norm": 310.2409, "avg_cosine": 0.9834, "reg_loss": 0.062}
[2025-11-28 02:15:26] {"epoch": 4, "step": 801, "loss": 0.8886, "avg_alpha": 0.019, "avg_norm": 308.0957, "avg_cosine": 0.9844, "reg_loss": 0.0663}
[2025-11-28 02:16:51] {"epoch": 4, "step": 901, "loss": 0.7921, "avg_alpha": 0.0205, "avg_norm": 310.9214, "avg_cosine": 0.9859, "reg_loss": 0.0752}
[2025-11-28 02:18:15] {"epoch": 4, "step": 1001, "loss": 0.6246, "avg_alpha": 0.0275, "avg_norm": 289.8985, "avg_cosine": 0.9818, "reg_loss": 0.0584}
[2025-11-28 02:19:38] {"epoch": 4, "step": 1101, "loss": 0.7024, "avg_alpha": 0.0344, "avg_norm": 292.0881, "avg_cosine": 0.9798, "reg_loss": 0.1319}
[2025-11-28 02:21:01] {"epoch": 4, "step": 1201, "loss": 0.6732, "avg_alpha": 0.0402, "avg_norm": 288.3754, "avg_cosine": 0.9826, "reg_loss": 0.1627}
[2025-11-28 02:22:24] {"epoch": 4, "step": 1301, "loss": 0.719, "avg_alpha": 0.0454, "avg_norm": 282.2555, "avg_cosine": 0.9683, "reg_loss": 0.162}
[2025-11-28 02:23:47] {"epoch": 4, "step": 1401, "loss": 0.8381, "avg_alpha": 0.0387, "avg_norm": 274.69, "avg_cosine": 0.9857, "reg_loss": 0.1292}
[2025-11-28 02:25:10] {"epoch": 4, "step": 1501, "loss": 0.765, "avg_alpha": 0.0359, "avg_norm": 282.9533, "avg_cosine": 0.9835, "reg_loss": 0.104}
[2025-11-28 02:26:33] {"epoch": 4, "step": 1601, "loss": 0.6487, "avg_alpha": 0.0374, "avg_norm": 266.1387, "avg_cosine": 0.9782, "reg_loss": 0.135}
[2025-11-28 02:27:57] {"epoch": 4, "step": 1701, "loss": 0.5968, "avg_alpha": 0.0399, "avg_norm": 295.3693, "avg_cosine": 0.9853, "reg_loss": 0.1116}
[2025-11-28 02:29:20] {"epoch": 4, "step": 1801, "loss": 0.5663, "avg_alpha": 0.0387, "avg_norm": 276.0865, "avg_cosine": 0.977, "reg_loss": 0.158}
[2025-11-28 02:30:44] {"epoch": 4, "step": 1901, "loss": 0.6286, "avg_alpha": 0.0341, "avg_norm": 286.8491, "avg_cosine": 0.9813, "reg_loss": 0.1497}
[2025-11-28 02:32:10] {"epoch": 4, "step": 2001, "loss": 0.4756, "avg_alpha": 0.0225, "avg_norm": 280.9808, "avg_cosine": 0.9709, "reg_loss": 0.0959}
[2025-11-28 02:33:34] {"epoch": 4, "step": 2101, "loss": 0.5393, "avg_alpha": 0.0328, "avg_norm": 262.7591, "avg_cosine": 0.9845, "reg_loss": 0.1309}
[2025-11-28 02:34:56] {"epoch": 4, "step": 2201, "loss": 0.4992, "avg_alpha": 0.0285, "avg_norm": 269.2115, "avg_cosine": 0.9804, "reg_loss": 0.1459}
[2025-11-28 02:36:19] {"epoch": 4, "step": 2301, "loss": 0.6027, "avg_alpha": 0.0281, "avg_norm": 275.1271, "avg_cosine": 0.9841, "reg_loss": 0.1201}
[2025-11-28 02:37:43] {"epoch": 4, "step": 2401, "loss": 0.6015, "avg_alpha": 0.026, "avg_norm": 273.0798, "avg_cosine": 0.9889, "reg_loss": 0.1121}
[2025-11-28 02:39:04] {"epoch": 4, "step": 2501, "loss": 0.6018, "avg_alpha": 0.0247, "avg_norm": 277.3833, "avg_cosine": 0.9804, "reg_loss": 0.089}
[2025-11-28 02:40:28] {"epoch": 4, "step": 2601, "loss": 0.52, "avg_alpha": 0.027, "avg_norm": 276.3837, "avg_cosine": 0.9787, "reg_loss": 0.0714}
[2025-11-28 02:41:50] {"epoch": 4, "step": 2701, "loss": 0.6358, "avg_alpha": 0.0162, "avg_norm": 266.2084, "avg_cosine": 0.9793, "reg_loss": 0.0845}
[2025-11-28 02:43:11] {"epoch": 4, "step": 2801, "loss": 0.4744, "avg_alpha": 0.0157, "avg_norm": 265.6397, "avg_cosine": 0.9841, "reg_loss": 0.0718}
[2025-11-28 02:44:32] {"epoch": 4, "step": 2901, "loss": 0.4996, "avg_alpha": 0.0323, "avg_norm": 253.5996, "avg_cosine": 0.9798, "reg_loss": 0.1425}
[2025-11-28 02:45:53] {"epoch": 4, "step": 3001, "loss": 0.582, "avg_alpha": 0.0205, "avg_norm": 248.4557, "avg_cosine": 0.9833, "reg_loss": 0.0951}
[2025-11-28 02:46:03] {"epoch": 4, "step": 3013, "loss": 0.5871, "avg_alpha": 0.0258, "avg_norm": 254.2692, "avg_cosine": 0.9838, "reg_loss": 0.0814}
[2025-11-28 02:46:07] Checkpoint saved: checkpoint_4
[2025-11-28 02:46:18] Evaluation Loss (Epoch 4): 0.6622293293476105
[2025-11-28 02:46:18] Eval Probes (Epoch 4): {"probe/avg_alpha": 0.0196, "probe/avg_norm": 254.8761, "probe/avg_cosine": 0.9966, "probe/reg_loss": 0.0989}
[2025-11-28 02:46:25] [Example] Pred: 2600 | GT: 300
[2025-11-28 02:46:25] [Example] Pred: 160 | GT: 240
[2025-11-28 02:46:26] [Example] Pred: It’s Meghan’s turn to pick up her team's coffee order.  She needs 2 drip coffees that are $2.25 each and one double shot espresso that’s $3.50.  She needs 2 lattes that are $4.00 and needs to add vanilla syrup to one of those for an additional $0.50.  She also needs 2 cold brew coffees that are $2.50 each and 1 cappuccino for $3.50.  How much is the coffee order?
<|start-latent|><|latent|><|latent|><|end-latent|><<2*4=8.00>>
<<2*2.5=5.00>>
<<2*2.5=5.00>>
<<2*2.5=5.00>>
<<5+8+5+2+2+3.50=28.50 | GT: 25
[2025-11-28 02:46:26] [Example] Pred: 10 | GT: 10
[2025-11-28 02:46:27] [Example] Pred: 45 | GT: 45
[2025-11-28 02:47:14] Accuracy on validation set: 75 / 500 = 0.15
[2025-11-28 02:47:14] CoT match on validation set: 0 / 500 = 0.0
[2025-11-28 02:47:14] {'eval/acc': 0.15, 'eval/cot_em': 0.0}
[2025-11-28 02:47:43] {"epoch": 5, "step": 3014, "loss": 0.6224, "avg_alpha": 0.0244, "avg_norm": 250.7718, "avg_cosine": 0.9793, "reg_loss": 0.1071}
[2025-11-28 02:49:06] {"epoch": 5, "step": 3114, "loss": 0.5275, "avg_alpha": 0.0199, "avg_norm": 240.7004, "avg_cosine": 0.9813, "reg_loss": 0.0981}
[2025-11-28 02:50:30] {"epoch": 5, "step": 3214, "loss": 0.5693, "avg_alpha": 0.0278, "avg_norm": 246.2952, "avg_cosine": 0.9808, "reg_loss": 0.0709}
[2025-11-28 02:51:54] {"epoch": 5, "step": 3314, "loss": 0.4902, "avg_alpha": 0.0397, "avg_norm": 255.1828, "avg_cosine": 0.9868, "reg_loss": 0.089}
[2025-11-28 02:53:19] {"epoch": 5, "step": 3414, "loss": 0.5564, "avg_alpha": 0.0268, "avg_norm": 246.0858, "avg_cosine": 0.9838, "reg_loss": 0.076}
[2025-11-28 02:54:46] {"epoch": 5, "step": 3514, "loss": 0.4705, "avg_alpha": 0.0386, "avg_norm": 241.1609, "avg_cosine": 0.9796, "reg_loss": 0.1072}
[2025-11-28 02:56:11] {"epoch": 5, "step": 3614, "loss": 0.4486, "avg_alpha": 0.0472, "avg_norm": 203.4792, "avg_cosine": 0.9716, "reg_loss": 0.1538}
[2025-11-28 02:57:36] {"epoch": 5, "step": 3714, "loss": 0.563, "avg_alpha": 0.0229, "avg_norm": 208.9719, "avg_cosine": 0.9804, "reg_loss": 0.0907}
[2025-11-28 02:59:01] {"epoch": 5, "step": 3814, "loss": 0.6363, "avg_alpha": 0.0273, "avg_norm": 216.6421, "avg_cosine": 0.9834, "reg_loss": 0.0626}
[2025-11-28 03:00:26] {"epoch": 5, "step": 3914, "loss": 0.3593, "avg_alpha": 0.0551, "avg_norm": 220.5688, "avg_cosine": 0.9863, "reg_loss": 0.1492}
[2025-11-28 03:01:50] {"epoch": 5, "step": 4014, "loss": 0.4357, "avg_alpha": 0.0528, "avg_norm": 217.3071, "avg_cosine": 0.9843, "reg_loss": 0.1146}
[2025-11-28 03:03:15] {"epoch": 5, "step": 4114, "loss": 0.3081, "avg_alpha": 0.0614, "avg_norm": 224.2159, "avg_cosine": 0.9821, "reg_loss": 0.1945}
[2025-11-28 03:04:39] {"epoch": 5, "step": 4214, "loss": 0.3558, "avg_alpha": 0.0423, "avg_norm": 202.014, "avg_cosine": 0.9778, "reg_loss": 0.1226}
[2025-11-28 03:06:04] {"epoch": 5, "step": 4314, "loss": 0.4662, "avg_alpha": 0.056, "avg_norm": 197.0938, "avg_cosine": 0.9701, "reg_loss": 0.1388}
[2025-11-28 03:07:29] {"epoch": 5, "step": 4414, "loss": 0.4253, "avg_alpha": 0.0628, "avg_norm": 198.5255, "avg_cosine": 0.965, "reg_loss": 0.1456}
[2025-11-28 03:08:53] {"epoch": 5, "step": 4514, "loss": 0.4314, "avg_alpha": 0.0693, "avg_norm": 212.2347, "avg_cosine": 0.9781, "reg_loss": 0.2189}
